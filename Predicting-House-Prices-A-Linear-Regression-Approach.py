# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nAe79svnnV2bkjXx_mA68lAA3sry-qME

# Importing Libraries:
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

"""# Loading the Dataset:"""

import pandas as pd

# URL of the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data"
# Full column names for the dataset
column_names = [
    "Per capita crime rate by town",
    "Proportion of residential land zoned for lots over 25,000 sq. ft.",
    "Proportion of non-retail business acres per town",
    "Charles River dummy variable (1 if tract bounds river; 0 otherwise)",
    "Nitric oxides concentration (parts per 10 million)",
    "Average number of rooms per dwelling",
    "Proportion of owner-occupied units built prior to 1940",
    "Weighted distances to five Boston employment centers",
    "Index of accessibility to radial highways",
    "Full-value property tax rate per $10,000",
    "Pupil-teacher ratio by town",
    "1000(Bk - 0.63)^2 where Bk is the proportion of Black residents by town",
    "Percentage of lower status of the population",
    "Median value of owner-occupied homes in $1000s"
]
# Reading the dataset into a DataFrame
data = pd.read_csv(url, delim_whitespace=True, names=column_names)

# Display the first few rows of the DataFrame
data.head()

data

"""# Exploratory Data Analysis (EDA)

## Summary Statistics
Summary statistics give us a quick overview of the distribution and key characteristics of each feature in the dataset. When we run data.describe(), it returns several important metrics:

Count: The number of non-missing values in each column.
Mean: The average value of each column.
Std (Standard Deviation): A measure of the amount of variation or dispersion of values in each column.
Min: The minimum value in each column.
25% (First Quartile): The value below which 25% of the data falls.
50% (Median): The middle value that separates the higher half from the lower half of the data.
75% (Third Quartile): The value below which 75% of the data falls.
Max: The maximum value in each column.
"""

# Display basic statistics of the dataset
data.describe()
"""
Explanation by Column (Examples)
Per capita crime rate by town (CRIM)

Count: There are 506 observations.
Mean: The average crime rate is 3.613524.
Std: The standard deviation is 8.601545, indicating high variability in crime rates.
Min: The minimum crime rate is 0.006320.
25%: 25% of the towns have a crime rate below 0.082045.
50% (Median): The median crime rate is 0.256510.
75%: 75% of the towns have a crime rate below 3.677083.
Max: The maximum crime rate is 88.976200, indicating a few towns with extremely high crime rates.

"""

"""
Calculate the Correlation Matrix:

data.corr() calculates the Pearson correlation coefficients between the features. The result is a correlation matrix that shows how strongly pairs of features are linearly related. The correlation coefficient ranges from -1 to 1, where:
1 indicates a perfect positive linear relationship,
-1 indicates a perfect negative linear relationship,
0 indicates no linear relationship.
Create a Heatmap:

sns.heatmap(corr_matrix, annot=True, cmap='coolwarm') creates a heatmap to visualize the correlation matrix.
corr_matrix is the data being visualized.
annot=True means that the correlation coefficient values will be annotated (displayed) on the heatmap.
cmap='coolwarm' specifies the color map. Here, 'coolwarm' is used, which displays positive correlations in warm colors (reds) and negative correlations in cool colors (blues).
"""
import seaborn as sns
import matplotlib.pyplot as plt

# Correlation matrix
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()
"""
Interpretation of the Heatmap
When you run this code, you will see a heatmap that represents the correlation matrix. Hereâ€™s how to interpret it:

Diagonal Elements: The diagonal elements (from top left to bottom right) will always be 1 because each feature is perfectly correlated with itself.
Positive Correlations: Positive values (closer to 1) indicate that as one feature increases, the other feature tends to also increase. These will appear in warm colors (reds/oranges).
Negative Correlations: Negative values (closer to -1) indicate that as one feature increases, the other feature tends to decrease. These will appear in cool colors (blues).
No Correlation: Values around 0 indicate little to no linear relationship between the features. These will be in neutral colors (white or very light shades).

Example Interpretation
Let's interpret a few example correlations that might appear in the heatmap:

Average number of rooms per dwelling (RM) vs. Median value of owner-occupied homes in $1000s (MEDV):

A high positive correlation (e.g., 0.7) would suggest that homes with more rooms tend to have higher median values.
"""

sns.pairplot(data)
plt.show()

"""


Example Insights from Pairplot
Let's look at some hypothetical insights you might gain from the pairplot:

Average number of rooms per dwelling (RM) vs. Median value of owner-occupied homes in $1000s (MEDV):

If you observe an upward trend, it suggests that homes with more rooms tend to have higher median values.
Percentage of lower status of the population (LSTAT) vs. Median value of owner-occupied homes in $1000s (MEDV):

If you see a downward trend, it suggests that as the percentage of lower status population increases, the median value of homes tends to decrease.
Nitric oxides concentration (NOX) vs. Weighted distances to five Boston employment centers (DIS):

If you see a downward trend, it suggests that areas closer to employment centers tend to have lower NOX concentrations.


Importance of Pairplot in EDA
A pairplot is extremely useful in EDA for several reasons:

Visualizing Relationships: It provides a comprehensive visual summary of the relationships between all pairs of variables.
Identifying Trends: It helps identify linear or non-linear trends between variables.
Detecting Outliers: Outliers can be easily spotted in scatterplots as points that deviate significantly from the trend.
Checking Data Distribution: The histograms on the diagonal help understand the distribution of individual variables, which is crucial for identifying skewness, kurtosis, and other distribution characteristics
"""

"""# Data Preprocessing:"""

# Checking for missing values
data.isnull().sum()

# Separate features (X) and target variable (y)
X = data.drop('Median value of owner-occupied homes in $1000s', axis=1)
y = data['Median value of owner-occupied homes in $1000s']


# Splitting data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""
X: The features.
y: The target variable.
test_size=0.2: Specifies that 20% of the data should be used for testing, and the remaining 80% for training.
random_state=42: Ensures reproducibility of the split. By setting a random state, you get the same split each time you run the code.
"""

X

y

"""# Training the Model:"""

# Creating and training the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

"""# Evaluating the Model:

### Example Scenario

Suppose we have a small dataset of actual house prices and predicted house prices:

- **Actual Prices (in $1000s)**: [200, 210, 190, 220, 230]
- **Predicted Prices (in $1000s)**: [195, 215, 180, 225, 235]

### Step-by-Step Calculation

#### 1. Mean Absolute Error (MAE)

First, we calculate the absolute errors and then the mean of those errors:

\[
\begin{align*}
\text{Absolute Errors} & : |200 - 195|, |210 - 215|, |190 - 180|, |220 - 225|, |230 - 235| \\
& : 5, 5, 10, 5, 5 \\
\text{MAE} & = \frac{5 + 5 + 10 + 5 + 5}{5} = \frac{30}{5} = 6
\end{align*}
\]

**Pros**: Simple to understand and interpret.
**Cons**: Does not penalize large errors as much as MSE and RMSE.

#### 2. Mean Squared Error (MSE)

Next, we calculate the squared errors and then the mean of those squared errors:

\[
\begin{align*}
\text{Squared Errors} & : (200 - 195)^2, (210 - 215)^2, (190 - 180)^2, (220 - 225)^2, (230 - 235)^2 \\
& : 25, 25, 100, 25, 25 \\
\text{MSE} & = \frac{25 + 25 + 100 + 25 + 25}{5} = \frac{200}{5} = 40
\end{align*}
\]

**Pros**: Penalizes larger errors more than MAE.
**Cons**: Units are squared, making interpretation less intuitive.

#### 3. Root Mean Squared Error (RMSE)

Finally, we take the square root of the MSE:

\[
\text{RMSE} = \sqrt{40} \approx 6.32
\]

**Pros**: Maintains the original units of the target variable.
**Cons**: More sensitive to large errors compared to MAE.

### Interpretation

- **MAE (Mean Absolute Error)**:
  - **Pros**: In our example, MAE is 6. This means that, on average, the model's predictions are off by $6000. It's straightforward to interpret.
  
  - **Cons**: MAE does not distinguish between small and large errors. For instance, whether the error is $5000 or $10,000, it only takes the absolute value without penalizing larger errors more heavily.

- **MSE (Mean Squared Error)**:
  - **Pros**: MSE is 40, which penalizes larger errors more. This is useful if you want to highlight significant deviations more prominently.
  - **Cons**: The units are squared ($1000^2), making it less intuitive to interpret directly in terms of dollars.

- **RMSE (Root Mean Squared Error)**:
  - **Pros**: RMSE is approximately 6.32, close to MAE but with a slight emphasis on larger errors. It is in the same units as the target variable, making it more interpretable.
  - **Cons**: Similar to MSE, it is sensitive to larger errors but provides a balance since it returns to the original units.

### Summary

- **MAE**: Best for simple interpretation and when you want an overall average error without penalizing large errors disproportionately.
- **MSE**: Best for scenarios where large errors are particularly undesirable, and you want to penalize them more heavily.
- **RMSE**: Combines the interpretability of MAE with the sensitivity to large errors of MSE, making it a balanced measure for average error magnitude.
"""

# Predicting on the test set
y_pred = model.predict(X_test)

# Calculating evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"Mean Absolute Error: {mae}")#Mean Absolute Error (MAE) is the average of the absolute differences between the predicted values and the actual values.
print(f"Mean Squared Error: {mse}")#Mean Squared Error (MSE) is the average of the squared differences between the predicted values and the actual values.
print(f"Root Mean Squared Error: {rmse}")#Root Mean Squared Error (RMSE) is the square root of MSE, which brings the units back to the original scale of the target variable

"""# Interpreting the Coefficients:"""

# Coefficients of the model
coefficients = pd.DataFrame(model.coef_, X.columns, columns=['Coefficient'])
print(coefficients)



"""
based on the coefficient values:

- Features with higher absolute coefficient values have a stronger influence on the predictions.
- A positive coefficient indicates that an increase in the feature value leads to an increase in the target variable (MEDV), while a negative coefficient indicates the opposite.
"""

r_squared = model.score(X_test,y_test)

r_squared

# Prepare input data
single_value = [[0.1, 25, 5, 1, 0.5, 6, 50, 5, 4, 300, 15, 400, 10]]

# Make prediction
prediction = model.predict(single_value)
print("Predicted Median House Value:", prediction)